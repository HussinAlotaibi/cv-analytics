{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "084cb55d",
   "metadata": {},
   "source": [
    "# CV Analytics Dashboard\n",
    "\n",
    "This notebook provides comprehensive CV analysis including:\n",
    "- Word Cloud generation\n",
    "- Top words analysis\n",
    "- Year mentions tracking\n",
    "- Keyword frequency analysis\n",
    "- Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ac97ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install python-docx pdfplumber PyPDF2 wordcloud nltk unidecode matplotlib pandas plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9d2720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and setup\n",
    "import os, re, io\n",
    "import pdfplumber, PyPDF2, docx\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"‚úÖ All libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6be2ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File reading functions\n",
    "def read_txt(path: str) -> str:\n",
    "    with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        return f.read()\n",
    "\n",
    "def read_docx(path: str) -> str:\n",
    "    d = docx.Document(path)\n",
    "    return \"\\n\".join(p.text for p in d.paragraphs)\n",
    "\n",
    "def read_pdf(path: str) -> str:\n",
    "    # Primary: pdfplumber; fallback: PyPDF2\n",
    "    try:\n",
    "        text = []\n",
    "        with pdfplumber.open(path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text.append(page.extract_text() or \"\")\n",
    "        out = \"\\n\".join(text)\n",
    "        if out.strip():\n",
    "            return out\n",
    "    except Exception as e:\n",
    "        print(f\"pdfplumber failed: {e}, trying PyPDF2...\")\n",
    "    \n",
    "    text = []\n",
    "    with open(path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for p in reader.pages:\n",
    "            text.append(p.extract_text() or \"\")\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def read_any(path: str) -> str:\n",
    "    ext = os.path.splitext(path.lower())[1]\n",
    "    if ext == \".pdf\":\n",
    "        return read_pdf(path)\n",
    "    elif ext == \".docx\":\n",
    "        return read_docx(path)\n",
    "    elif ext == \".txt\":\n",
    "        return read_txt(path)\n",
    "    else:\n",
    "        # Fallback to text reading\n",
    "        try:\n",
    "            return read_txt(path)\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "print(\"‚úÖ File reading functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012cb448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and text processing\n",
    "_TOKEN_RE = re.compile(r\"[A-Za-z0-9]+(?:[-_][A-Za-z0-9]+)*\")\n",
    "EN_STOP = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Custom stopwords for CV analysis\n",
    "CUSTOM_STOP = {\n",
    "    'cv', 'resume', 'responsible', 'worked', 'using', 'project', 'projects', \n",
    "    'tool', 'performed', 'objective', 'summary', 'include', 'data', \n",
    "    'management', 'experience', 'senior', 'lead', 'team', 'develop', \n",
    "    'developed', 'design', 'designed', 'implement', 'implemented', 'build', \n",
    "    'built', 'created', 'skills', 'skill', 'abilities', 'ability', \n",
    "    'proficient', 'knowledge', 'strong', 'understanding', 'etc'\n",
    "}\n",
    "\n",
    "def tokenize(text: str, extra_stop: set = None):\n",
    "    text = unidecode(text or \"\")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    tokens = [t.lower() for t in _TOKEN_RE.findall(text)]\n",
    "    \n",
    "    # Remove pure numbers\n",
    "    tokens = [t for t in tokens if not t.isdigit()]\n",
    "    \n",
    "    # Apply stopwords\n",
    "    stop = EN_STOP.copy()\n",
    "    stop.update(CUSTOM_STOP)\n",
    "    if extra_stop:\n",
    "        stop.update({w.lower() for w in extra_stop})\n",
    "    \n",
    "    tokens = [t for t in tokens if t not in stop and len(t) > 2]\n",
    "    return tokens, text\n",
    "\n",
    "print(\"‚úÖ Tokenization functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year extraction functions\n",
    "YEAR_RE = re.compile(r\"\\b(19\\d{2}|20\\d{2})\\b|\\b(19\\d{2}|20\\d{2})\\s*(?:-|‚Äì|to)\\s*(19\\d{2}|20\\d{2}|present|now)\\b\", re.IGNORECASE)\n",
    "\n",
    "def extract_year_mentions(raw_text: str):\n",
    "    years = []\n",
    "    found_patterns = YEAR_RE.findall(raw_text)\n",
    "    \n",
    "    for pattern in found_patterns:\n",
    "        for year_str in pattern:\n",
    "            if year_str and year_str.lower() not in (\"present\", \"now\"):\n",
    "                try:\n",
    "                    y = int(year_str)\n",
    "                    if 1950 <= y <= datetime.now().year + 1:\n",
    "                        years.append(y)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "    return years\n",
    "\n",
    "print(\"‚úÖ Year extraction functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e38861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main CV Analysis Class\n",
    "class CVAnalyzer:\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "        self.file_name = os.path.basename(file_path)\n",
    "        self.raw_text = read_any(file_path)\n",
    "        self.tokens, self.normalized_text = tokenize(self.raw_text)\n",
    "        self.word_counts = Counter(self.tokens)\n",
    "        self.year_mentions = extract_year_mentions(self.raw_text)\n",
    "        \n",
    "    def get_summary(self):\n",
    "        return {\n",
    "            'file_name': self.file_name,\n",
    "            'total_words': len(self.tokens),\n",
    "            'unique_words': len(set(self.tokens)),\n",
    "            'raw_text_length': len(self.raw_text),\n",
    "            'years_mentioned': len(self.year_mentions)\n",
    "        }\n",
    "    \n",
    "    def generate_word_cloud(self, max_words=300, width=1600, height=900):\n",
    "        # Merge all stopwords\n",
    "        wc_stop = STOPWORDS.union(EN_STOP).union({w.lower() for w in CUSTOM_STOP})\n",
    "        \n",
    "        cloud = WordCloud(\n",
    "            width=width, height=height,\n",
    "            background_color=\"white\",\n",
    "            max_words=max_words,\n",
    "            collocations=False,\n",
    "            stopwords=wc_stop\n",
    "        ).generate(\" \".join(self.tokens))\n",
    "        \n",
    "        plt.figure(figsize=(16, 9))\n",
    "        plt.imshow(cloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Word Cloud ‚Äî {self.file_name}\", fontsize=16, pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return cloud\n",
    "    \n",
    "    def plot_top_words(self, n=20):\n",
    "        top_words = self.word_counts.most_common(n)\n",
    "        words = [word for word, count in top_words]\n",
    "        counts = [count for word, count in top_words]\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        bars = plt.bar(range(len(words)), counts, color='skyblue')\n",
    "        plt.xlabel(\"Words\", fontsize=12)\n",
    "        plt.ylabel(\"Frequency\", fontsize=12)\n",
    "        plt.title(f\"Top {n} Words in {self.file_name}\", fontsize=14)\n",
    "        plt.xticks(range(len(words)), words, rotation=45, ha=\"right\")\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, count in zip(bars, counts):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "                    str(count), ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return top_words\n",
    "    \n",
    "    def plot_year_mentions(self):\n",
    "        if not self.year_mentions:\n",
    "            print(\"No year mentions found in the CV.\")\n",
    "            return\n",
    "        \n",
    "        year_counts = Counter(self.year_mentions)\n",
    "        years = sorted(year_counts.keys())\n",
    "        counts = [year_counts[year] for year in years]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(years, counts, marker='o', linewidth=2, markersize=8, color='orange')\n",
    "        plt.xlabel(\"Year\", fontsize=12)\n",
    "        plt.ylabel(\"Number of Mentions\", fontsize=12)\n",
    "        plt.title(f\"Year Mentions in {self.file_name}\", fontsize=14)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on points\n",
    "        for year, count in zip(years, counts):\n",
    "            plt.text(year, count + 0.1, str(count), ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return year_counts\n",
    "    \n",
    "    def analyze_keywords(self, keyword_lists):\n",
    "        \"\"\"Analyze keyword frequency for different categories\"\"\"\n",
    "        def count_keywords(text: str, keywords: list):\n",
    "            text_lower = text.lower()\n",
    "            counts = {}\n",
    "            for keyword in keywords:\n",
    "                pattern = r\"\\b\" + re.escape(keyword.lower()) + r\"\\b\"\n",
    "                counts[keyword] = len(re.findall(pattern, text_lower))\n",
    "            return counts\n",
    "        \n",
    "        results = {}\n",
    "        all_keywords = []\n",
    "        all_counts = []\n",
    "        \n",
    "        for category, keywords in keyword_lists.items():\n",
    "            results[category] = count_keywords(self.raw_text, keywords)\n",
    "            for keyword, count in results[category].items():\n",
    "                all_keywords.append(f\"{category}: {keyword}\")\n",
    "                all_counts.append(count)\n",
    "        \n",
    "        # Sort by count\n",
    "        sorted_data = sorted(zip(all_keywords, all_counts), key=lambda x: x[1], reverse=True)\n",
    "        sorted_keywords, sorted_counts = zip(*sorted_data) if sorted_data else ([], [])\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, max(8, len(sorted_keywords) * 0.4)))\n",
    "        bars = plt.barh(range(len(sorted_keywords)), sorted_counts, color='lightcoral')\n",
    "        plt.xlabel(\"Frequency\", fontsize=12)\n",
    "        plt.ylabel(\"Keywords\", fontsize=12)\n",
    "        plt.title(f\"Keyword Analysis for {self.file_name}\", fontsize=14)\n",
    "        plt.yticks(range(len(sorted_keywords)), sorted_keywords)\n",
    "        plt.gca().invert_yaxis()\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, count in enumerate(sorted_counts):\n",
    "            plt.text(count + 0.1, i, str(count), va='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def create_summary_table(self):\n",
    "        summary = self.get_summary()\n",
    "        top_10 = self.word_counts.most_common(10)\n",
    "        \n",
    "        print(f\"üìÑ CV Analysis Summary for: {summary['file_name']}\")\n",
    "        print(f\"üìä Total Words: {summary['total_words']:,}\")\n",
    "        print(f\"üî§ Unique Words: {summary['unique_words']:,}\")\n",
    "        print(f\"üìù Raw Text Length: {summary['raw_text_length']:,} characters\")\n",
    "        print(f\"üìÖ Years Mentioned: {summary['years_mentioned']}\")\n",
    "        print(\"\\nüîù Top 10 Words:\")\n",
    "        for i, (word, count) in enumerate(top_10, 1):\n",
    "            print(f\"{i:2d}. {word:<15} ({count:>3} times)\")\n",
    "\n",
    "print(\"‚úÖ CVAnalyzer class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a323382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File upload and analysis\n",
    "# Replace this path with your CV file path\n",
    "CV_FILE_PATH = r\"c:\\Users\\Hussain\\Downloads\\Sattam Alotaibi - Resume  2025.pdf\"\n",
    "\n",
    "# Alternative: use file dialog for interactive selection\n",
    "# from tkinter import filedialog\n",
    "# import tkinter as tk\n",
    "# root = tk.Tk()\n",
    "# root.withdraw()\n",
    "# CV_FILE_PATH = filedialog.askopenfilename(\n",
    "#     title=\"Select CV file\",\n",
    "#     filetypes=[(\"All supported\", \"*.pdf *.docx *.txt\"), (\"PDF files\", \"*.pdf\"), (\"Word files\", \"*.docx\"), (\"Text files\", \"*.txt\")]\n",
    "# )\n",
    "\n",
    "if os.path.exists(CV_FILE_PATH):\n",
    "    print(f\"‚úÖ Loading CV: {os.path.basename(CV_FILE_PATH)}\")\n",
    "    analyzer = CVAnalyzer(CV_FILE_PATH)\n",
    "    print(f\"‚úÖ CV loaded successfully! Extracted {len(analyzer.raw_text)} characters of text.\")\n",
    "else:\n",
    "    print(f\"‚ùå File not found: {CV_FILE_PATH}\")\n",
    "    print(\"Please update the CV_FILE_PATH variable with the correct path to your CV file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790858c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive analysis\n",
    "if 'analyzer' in locals():\n",
    "    # 1. Summary Statistics\n",
    "    print(\"=\" * 60)\n",
    "    analyzer.create_summary_table()\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"‚ùå Analyzer not loaded. Please run the previous cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0c676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Word Cloud Generation\n",
    "if 'analyzer' in locals():\n",
    "    print(\"üé® Generating Word Cloud...\")\n",
    "    word_cloud = analyzer.generate_word_cloud(max_words=300)\n",
    "else:\n",
    "    print(\"‚ùå Analyzer not loaded. Please run the file loading cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0923ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Top Words Analysis\n",
    "if 'analyzer' in locals():\n",
    "    print(\"üìä Analyzing Top Words...\")\n",
    "    top_words = analyzer.plot_top_words(n=20)\n",
    "else:\n",
    "    print(\"‚ùå Analyzer not loaded. Please run the file loading cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0be28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Year Mentions Analysis\n",
    "if 'analyzer' in locals():\n",
    "    print(\"üìÖ Analyzing Year Mentions...\")\n",
    "    year_data = analyzer.plot_year_mentions()\n",
    "else:\n",
    "    print(\"‚ùå Analyzer not loaded. Please run the file loading cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e194592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Keyword Analysis\n",
    "if 'analyzer' in locals():\n",
    "    # Define keyword categories for analysis\n",
    "    keyword_lists = {\n",
    "        \"Programming Languages\": [\"python\", \"sql\", \"java\", \"javascript\", \"r\", \"scala\", \"c++\", \"c#\"],\n",
    "        \"Data & Analytics\": [\"data\", \"analytics\", \"machine learning\", \"ai\", \"statistics\", \"visualization\", \"tableau\", \"power bi\"],\n",
    "        \"Cloud Platforms\": [\"aws\", \"azure\", \"gcp\", \"google cloud\", \"cloud\"],\n",
    "        \"Databases\": [\"mysql\", \"postgresql\", \"mongodb\", \"oracle\", \"sql server\", \"redis\"],\n",
    "        \"Management Skills\": [\"leadership\", \"project management\", \"team\", \"strategy\", \"planning\"],\n",
    "        \"Technical Skills\": [\"api\", \"microservices\", \"docker\", \"kubernetes\", \"git\", \"ci/cd\"]\n",
    "    }\n",
    "    \n",
    "    print(\"üîç Analyzing Keywords by Category...\")\n",
    "    keyword_results = analyzer.analyze_keywords(keyword_lists)\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\nüìã Detailed Keyword Results:\")\n",
    "    for category, results in keyword_results.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for keyword, count in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "            if count > 0:\n",
    "                print(f\"  ‚Ä¢ {keyword}: {count}\")\n",
    "else:\n",
    "    print(\"‚ùå Analyzer not loaded. Please run the file loading cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1d5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Export Results to CSV\n",
    "if 'analyzer' in locals():\n",
    "    print(\"üíæ Exporting results to CSV...\")\n",
    "    \n",
    "    # Export top words\n",
    "    top_words_df = pd.DataFrame(analyzer.word_counts.most_common(50), columns=['Word', 'Frequency'])\n",
    "    top_words_df.to_csv('cv_top_words.csv', index=False)\n",
    "    \n",
    "    # Export year mentions\n",
    "    if analyzer.year_mentions:\n",
    "        year_counts = Counter(analyzer.year_mentions)\n",
    "        years_df = pd.DataFrame(list(year_counts.items()), columns=['Year', 'Mentions'])\n",
    "        years_df = years_df.sort_values('Year')\n",
    "        years_df.to_csv('cv_year_mentions.csv', index=False)\n",
    "    \n",
    "    # Export keyword analysis\n",
    "    if 'keyword_results' in locals():\n",
    "        keyword_data = []\n",
    "        for category, results in keyword_results.items():\n",
    "            for keyword, count in results.items():\n",
    "                keyword_data.append({'Category': category, 'Keyword': keyword, 'Count': count})\n",
    "        \n",
    "        keywords_df = pd.DataFrame(keyword_data)\n",
    "        keywords_df.to_csv('cv_keyword_analysis.csv', index=False)\n",
    "    \n",
    "    print(\"‚úÖ Results exported to CSV files:\")\n",
    "    print(\"  ‚Ä¢ cv_top_words.csv\")\n",
    "    print(\"  ‚Ä¢ cv_year_mentions.csv\")\n",
    "    print(\"  ‚Ä¢ cv_keyword_analysis.csv\")\n",
    "else:\n",
    "    print(\"‚ùå Analyzer not loaded. Please run the file loading cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508dc5a5",
   "metadata": {},
   "source": [
    "## üéØ Analysis Complete!\n",
    "\n",
    "This notebook has analyzed your CV and provided:\n",
    "\n",
    "1. **üìä Summary Statistics** - Total words, unique words, character count\n",
    "2. **üé® Word Cloud** - Visual representation of most frequent words\n",
    "3. **üìà Top Words Chart** - Bar chart of most common terms\n",
    "4. **üìÖ Year Mentions** - Timeline of years mentioned in the CV\n",
    "5. **üîç Keyword Analysis** - Frequency of technical and professional terms\n",
    "6. **üíæ CSV Exports** - Data exported for further analysis\n",
    "\n",
    "### üîß Customization Options:\n",
    "\n",
    "- **Add more keywords**: Modify the `keyword_lists` dictionary\n",
    "- **Change word cloud settings**: Adjust `max_words`, `width`, `height` parameters\n",
    "- **Filter stopwords**: Add custom words to `CUSTOM_STOP` set\n",
    "- **Adjust chart parameters**: Modify `n` parameter for top words count\n",
    "\n",
    "### üìù Next Steps:\n",
    "\n",
    "1. Review the generated visualizations\n",
    "2. Use the CSV exports for additional analysis\n",
    "3. Customize the keyword categories for your specific needs\n",
    "4. Run the analysis on multiple CVs for comparison"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
